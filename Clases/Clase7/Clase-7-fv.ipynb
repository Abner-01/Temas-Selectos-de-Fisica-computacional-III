{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T01:21:08.810200Z",
     "start_time": "2020-03-04T01:21:08.804251Z"
    }
   },
   "source": [
    "<font size=4 color='blue'>\n",
    "\n",
    "# <center> Clase 7, noviembre 11 del 2020 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T23:31:40.881412Z",
     "start_time": "2020-10-11T23:31:40.873889Z"
    }
   },
   "source": [
    "<font size=4 color='blue'>\n",
    "\n",
    "# <center> Study topic: Mortality from diabetes </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'>\n",
    "    \n",
    "## Information about the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\n",
    "Evolution of diabetes after one year.\n",
    "    \n",
    "In the present work, we characterize diabetes with the following ten features: age, sex, body mass index, mean blood pressure, and six measurements of blood serum (S1, S2, S3, S4, S5, S6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'>\n",
    "    \n",
    "## Quantification of this information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\n",
    "Information is available on 442 patients (m = 442). The response of interest, Y, is a quantitative measure of disease progression one year after the start of the study. Y values vary between 25 and 346â€‹.\n",
    "\n",
    "Information source: [diabetes data](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html)    \n",
    "\n",
    "Original paper: [Least-Angle-Regression_2004](./Literatura/Least-Angle-Regression_2004.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:36.157661Z",
     "start_time": "2020-11-11T00:35:34.979835Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:36.177930Z",
     "start_time": "2020-11-11T00:35:36.159152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data are available in the file diabetes.csv\n",
    "\n",
    "df = pd.read_csv('diabetes.csv', sep ='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:36.194483Z",
     "start_time": "2020-11-11T00:35:36.179617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Showing the first 5 samples (features and target Y)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:36.238174Z",
     "start_time": "2020-11-11T00:35:36.196016Z"
    }
   },
   "outputs": [],
   "source": [
    "# The describe() method generates a table with statistical information \n",
    "# for each of the features and the target.\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms are created for each of the features that characterize patients with diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:36.835611Z",
     "start_time": "2020-11-11T00:35:36.239452Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8)) \n",
    "\n",
    "ax1 = plt.subplot(2,4,1)\n",
    "ax2 = plt.subplot(2,4,2)\n",
    "ax3 = plt.subplot(2,4,3)\n",
    "ax4 = plt.subplot(2,4,4)\n",
    "\n",
    "ax1.hist(df.AGE, bins=30, color='green',edgecolor='purple', alpha=0.5)\n",
    "ax1.set_xlabel('Age (years)', size=15)\n",
    "ax1.set_ylabel('Frequency', size=15)\n",
    "\n",
    "ax2.hist(df.SEX, bins=30, color='orange',edgecolor='purple', alpha=0.5)\n",
    "ax2.set_xlabel('Sex', size=15)\n",
    "\n",
    "ax3.hist(df.BMI, bins=30, color='red',edgecolor='purple', alpha=0.5)\n",
    "ax3.set_xlabel('Body_mass_index', size=15)\n",
    "\n",
    "ax4.hist(df.BP, bins=30, color='blue',edgecolor='purple', alpha=0.5)\n",
    "ax4.set_xlabel('Average_blood_pressure', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:37.329866Z",
     "start_time": "2020-11-11T00:35:36.837416Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8)) \n",
    "\n",
    "ax1 = plt.subplot(2,4,1)\n",
    "ax2 = plt.subplot(2,4,2)\n",
    "ax3 = plt.subplot(2,4,3)\n",
    "ax4 = plt.subplot(2,4,4)\n",
    "\n",
    "ax1.hist(df.S1, bins=30, color='green',edgecolor='purple', alpha=0.5)\n",
    "ax1.set_xlabel('S1', size=15)\n",
    "ax1.set_ylabel('Frequency', size=15)\n",
    "\n",
    "ax2.hist(df.S2, bins=30, color='orange',edgecolor='purple', alpha=0.5)\n",
    "ax2.set_xlabel('S2', size=15)\n",
    "\n",
    "ax3.hist(df.S3, bins=30, color='red',edgecolor='purple', alpha=0.5)\n",
    "ax3.set_xlabel('S3', size=15)\n",
    "\n",
    "ax4.hist(df.S4, bins=30, color='blue',edgecolor='purple', alpha=0.5)\n",
    "ax4.set_xlabel('S4', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:37.704964Z",
     "start_time": "2020-11-11T00:35:37.331686Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8)) \n",
    "\n",
    "ax1 = plt.subplot(2,3,1)\n",
    "ax2 = plt.subplot(2,3,2)\n",
    "ax3 = plt.subplot(2,3,3)\n",
    "\n",
    "ax1.hist(df.S5, bins=30, color='green',edgecolor='purple', alpha=0.5)\n",
    "ax1.set_xlabel('S5', size=15)\n",
    "ax1.set_ylabel('Frequency', size=15)\n",
    "\n",
    "ax2.hist(df.S6, bins=30, color='orange',edgecolor='purple', alpha=0.5)\n",
    "ax2.set_xlabel('S6', size=15)\n",
    "\n",
    "ax3.hist(df.Y, bins=30, color='purple',edgecolor='black', alpha=0.5)\n",
    "ax3.set_xlabel('Y', size=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T20:21:23.071130Z",
     "start_time": "2020-10-13T20:21:23.062454Z"
    }
   },
   "source": [
    "<font size=4>\n",
    "\n",
    "To remove any possible correlation between the samples (the rows of the DataFrame), they are randomly reordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:37.712173Z",
     "start_time": "2020-11-11T00:35:37.707248Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "The original samples are divided into 2 sets: 90% for training and 10% for making inferences (predictions) with what has been learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:37.725189Z",
     "start_time": "2020-11-11T00:35:37.713754Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_ratio = int((1.0-test_ratio)*len(df.values[:,:]))\n",
    "\n",
    "df_train = df.iloc[0:train_ratio,:]\n",
    "df_test  = df.iloc[train_ratio:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:37.737228Z",
     "start_time": "2020-11-11T00:35:37.727226Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\n",
    "To work with the models, it is required that all the variables have the same order of magnitude. For this reason, their values are normalized in the samples that are going to be used in training, both the features (X) and the target (Y):\n",
    "\n",
    "$$x_{norm} = \\dfrac{x-\\bar{x}}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:37.767216Z",
     "start_time": "2020-11-11T00:35:37.738462Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_norm = (df_train - df_train.mean()) / df_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:37.778050Z",
     "start_time": "2020-11-11T00:35:37.768606Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_norm = (df_test - df_train.mean()) / df_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "Histograms of the variables to be used in the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:38.276209Z",
     "start_time": "2020-11-11T00:35:37.780174Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8)) \n",
    "\n",
    "ax1 = plt.subplot(2,4,1)\n",
    "ax2 = plt.subplot(2,4,2)\n",
    "ax3 = plt.subplot(2,4,3)\n",
    "ax4 = plt.subplot(2,4,4)\n",
    "\n",
    "ax1.hist(df_train_norm.AGE, bins=30, color='green',edgecolor='purple', alpha=0.5)\n",
    "ax1.set_xlabel('x1(Age)', size=15)\n",
    "ax1.set_ylabel('Frequency', size=15)\n",
    "\n",
    "ax2.hist(df_train_norm.SEX, bins=30, color='orange',edgecolor='purple', alpha=0.5)\n",
    "ax2.set_xlabel('x2(Sex)', size=15)\n",
    "\n",
    "ax3.hist(df_train_norm.BMI, bins=30, color='red',edgecolor='purple', alpha=0.5)\n",
    "ax3.set_xlabel('x3(Body_mass_index)', size=15)\n",
    "\n",
    "ax4.hist(df_train_norm.BP, bins=30, color='blue',edgecolor='purple', alpha=0.5)\n",
    "ax4.set_xlabel('x4(Average_blood_pressure)', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:38.803209Z",
     "start_time": "2020-11-11T00:35:38.277795Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8)) \n",
    "\n",
    "ax1 = plt.subplot(2,4,1)\n",
    "ax2 = plt.subplot(2,4,2)\n",
    "ax3 = plt.subplot(2,4,3)\n",
    "ax4 = plt.subplot(2,4,4)\n",
    "\n",
    "ax1.hist(df_train_norm.S1, bins=30, color='green',edgecolor='purple', alpha=0.5)\n",
    "ax1.set_xlabel('x5(S1)', size=15)\n",
    "ax1.set_ylabel('Frequency', size=15)\n",
    "\n",
    "ax2.hist(df_train_norm.S2, bins=30, color='orange',edgecolor='purple', alpha=0.5)\n",
    "ax2.set_xlabel('x6(S2)', size=15)\n",
    "\n",
    "ax3.hist(df_train_norm.S3, bins=30, color='red',edgecolor='purple', alpha=0.5)\n",
    "ax3.set_xlabel('x7(S3)', size=15)\n",
    "\n",
    "ax4.hist(df_train_norm.S4, bins=30, color='blue',edgecolor='purple', alpha=0.5)\n",
    "ax4.set_xlabel('x8(S4)', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:39.161834Z",
     "start_time": "2020-11-11T00:35:38.805286Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8)) \n",
    "\n",
    "ax1 = plt.subplot(2,3,1)\n",
    "ax2 = plt.subplot(2,3,2)\n",
    "ax3 = plt.subplot(2,3,3)\n",
    "\n",
    "ax1.hist(df_train_norm.S5, bins=30, color='green',edgecolor='purple', alpha=0.5)\n",
    "ax1.set_xlabel('x9(S5)', size=15)\n",
    "ax1.set_ylabel('Frequency', size=15)\n",
    "\n",
    "ax2.hist(df_train_norm.S6, bins=30, color='orange',edgecolor='purple', alpha=0.5)\n",
    "ax2.set_xlabel('x10(S6)', size=15)\n",
    "\n",
    "ax3.hist(df_train_norm.Y, bins=30, color='purple',edgecolor='black', alpha=0.5)\n",
    "ax3.set_xlabel('Y', size=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "X and Y values are extracted from the columns of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:39.167248Z",
     "start_time": "2020-11-11T00:35:39.163909Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x = df_train_norm.values[:,:-1]\n",
    "train_y = df_train_norm.values[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:39.179496Z",
     "start_time": "2020-11-11T00:35:39.169182Z"
    }
   },
   "outputs": [],
   "source": [
    "test_x = df_test_norm.values[:,:-1]\n",
    "test_y = df_test_norm.values[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:39.192106Z",
     "start_time": "2020-11-11T00:35:39.181116Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "# <center> Modeling different learning systems </center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'>\n",
    "\n",
    "# <center> Implemented using the Keras framework as frontend </center>\n",
    "\n",
    "\n",
    "<font size=4 color='mediumvioletred'>\n",
    "   \n",
    "[Keras](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:43.115745Z",
     "start_time": "2020-11-11T00:35:39.193338Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sequential](https://keras.io/guides/sequential_model/)\n",
    "\n",
    "[layers](https://keras.io/api/layers/): [Dense](https://keras.io/api/layers/core_layers/dense/), [Activation](https://keras.io/api/layers/activations/#relu-function)\n",
    "\n",
    "[Optimizers](https://keras.io/api/optimizers/#available-optimizers)\n",
    "\n",
    "[utils](https://keras.io/api/utils/)\n",
    "\n",
    "[Keras API reference](https://keras.io/api/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "\n",
    "The features that determine the phenomenon are described by the vector  $X = (x_1, x_2, x_3, ...x_k,...x_K)$ and is called a feature vector.\n",
    "    \n",
    "The model assumes that the output $y$ varies linearly with each feature\n",
    "    $$ F(X) = \\sum_{k=1}^K w_k*x_k + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:46:25.160502Z",
     "start_time": "2020-10-19T18:46:25.155595Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "First model: The output $y$ depends linearly on each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:43.479713Z",
     "start_time": "2020-11-11T00:35:43.117062Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    def  __init__ (self,sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        print(\"It has\", self.num_layers, \"layers,\")\n",
    "        self.sizes = sizes\n",
    "        print(\"with the following number of nodes per layer\",self.sizes)\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, x_of_sample):\n",
    "        \"\"\"Return the output of the network F(x_of_sample) \"\"\"        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            x_of_sample = sigmoid(np.dot(w, x_of_sample)+b)\n",
    "        return x_of_sample\n",
    "    \n",
    "    def graph(self,sizes):\n",
    "        a=[]\n",
    "        ps={}\n",
    "        Q = nx.Graph()\n",
    "        for i in range(len(sizes)):\n",
    "            Qi=nx.Graph()    \n",
    "            n=sizes[i]\n",
    "            nodos=np.arange(n)\n",
    "            Qi.add_nodes_from(nodos)\n",
    "            l_i=Qi.nodes\n",
    "            Q = nx.union(Q, Qi, rename = (None, 'Q%i-'%i))\n",
    "            if len(l_i)==1:\n",
    "                ps['Q%i-0'%i]=[i/(len(sizes)), 1/2]\n",
    "            else:\n",
    "                for j in range(len(l_i)+1):\n",
    "                    ps['Q%i-%i'%(i,j)]=[i/(len(sizes)),(1/(len(l_i)*len(l_i)))+(j/(len(l_i)))]\n",
    "            a.insert(i,Qi)\n",
    "        for i in range(len(a)-1):\n",
    "            for j in range(len(a[i])):\n",
    "                for k in range(len(a[i+1])):\n",
    "                    Q.add_edge('Q%i-%i' %(i,j),'Q%i-%i' %(i+1,k))            \n",
    "        nx.draw(Q, pos = ps)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:43.616943Z",
     "start_time": "2020-11-11T00:35:43.480934Z"
    }
   },
   "outputs": [],
   "source": [
    "n_x = train_x.shape[1] \n",
    "n_y = train_y.shape[1]\n",
    "    \n",
    "layers = [n_x, n_y]\n",
    "net = Network(layers)\n",
    "net.graph(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Definition of architecture. \n",
    "    \n",
    "It includes the initialization of weights and biases, as well as the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:43.661595Z",
     "start_time": "2020-11-11T00:35:43.618469Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "input_nodes = n_x     # The input layer has n_x nodes\n",
    "output_nodes = n_y    # The output layer has n_y nodes\n",
    "\n",
    "linear_model = Sequential()\n",
    "\n",
    "# For the first layer, you need to indicate its input layer, which corresponds to\n",
    "# the input layer of the network.\n",
    "\n",
    "linear_model.add(Dense(output_nodes,  kernel_initializer='uniform', bias_initializer='zeros', \\\n",
    "                input_dim=input_nodes, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    \n",
    "  **Parameter Initialization** Strategies\n",
    "\n",
    "Training algorithms for deep learning models are usually iterative in nature and thus require the user to specify some initial point from which to begin the iterations. Moreover, training deep models is a sufficiently difficult task that most algorithms are strongly affected by the choice of initialization. The initial point can determine whether the algorithm converges at all, with some initial points being so unstable that the algorithm encounters numerical difficulties and fails altogether. When learning does converge, the initial point can determine how quickly learning converges and whether it converges to a point with high or low cost. Also, points of comparable cost can have wildly varying generalization error, and the initial point can affect the generalization as well.\n",
    "\n",
    "     Yoshua Bengio, Aaron Courville, Ian Goodfellow. Deep Learning. pp 301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:55:20.867257Z",
     "start_time": "2020-10-19T18:55:20.859036Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "Architecture Summary and Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:43.860403Z",
     "start_time": "2020-11-11T00:35:43.665173Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(linear_model, to_file='linear_model.png', show_shapes=True, rankdir='TB', \n",
    "           expand_nested=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(plot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:43.871666Z",
     "start_time": "2020-11-11T00:35:43.865348Z"
    }
   },
   "outputs": [],
   "source": [
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5  color='blue'>\n",
    "    \n",
    "Compiling the model. Includes the optimizer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:44.013867Z",
     "start_time": "2020-11-11T00:35:43.874027Z"
    }
   },
   "outputs": [],
   "source": [
    "# We define the optimizing function and their hyperparameters: learining rate(lr), \n",
    "# decay, momentum and nesterov (whether to apply Nesterov gradient)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, momentum=0.0, nesterov=False)\n",
    "\n",
    "linear_model.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\n",
    "**Stochastic gradient descent (SGD)** is an extension of the gradient descent algorithm.\n",
    "\n",
    "A recurring problem in machine learning is that large training sets are necessary for good generalization, but large training sets are also more computationally expensive.\n",
    "\n",
    "The insight of stochastic gradient descent is that the gradient is an expectation. The expectation may be approximately estimated using a small set of samples. Specifically, on each step of the algorithm, we can sample a minibatch of examples.\n",
    "    \n",
    "    Yoshua Bengio, Aaron Courville, Ian Goodfellow. Deep Learning. pp 151\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Training the learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.317907Z",
     "start_time": "2020-11-11T00:35:44.015795Z"
    }
   },
   "outputs": [],
   "source": [
    "# 10 % of the training data will be used to validate the training\n",
    "validation_portion = 0.1\n",
    "epochs = 600\n",
    "\n",
    "history = linear_model.fit(train_x, train_y, epochs=epochs, validation_split = validation_portion, verbose=1)\n",
    "\n",
    "# the \"history\" object contains the information generated during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Plots of the cost function versus epoch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.473920Z",
     "start_time": "2020-11-11T00:35:59.319596Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history.history['loss'], color='red')\n",
    "plt.plot(history.history['val_loss'], color='green')\n",
    "plt.title('Cost function')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cost_train', 'cost_validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\n",
    "The factors determining how well a machine learning algorithm will perform are its ability to:\n",
    "\n",
    "1. Make the training error small.\n",
    "2. Make the gap between training and test error small.\n",
    "\n",
    "**Underfitting** occurs when the model is not able to obtain a sufficiently low error value on the training set.\n",
    "\n",
    "**Overfitting** occurs when the gap between the training error and test error is too large.\n",
    "<img src='images\\bengio.png'>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    Yoshua Bengio, Aaron Courville, Ian Goodfellow. Deep Learning. pp 111\n",
    "    \n",
    "    \n",
    "<font size=4>\n",
    "    \n",
    "When we compare the training and validation errors, we want to be mindful of two common situations. First, we want to watch out for cases when our training error and validation error are both substantial but there is a little gap between them. If the model is unable to reduce the training error, that could mean that our model is too simple (i.e., insufficiently expressive) to capture the pattern that we are trying to model. Moreover, since the generalization gap between our training and validation errors is small, we have reason to believe that we could get away with a more complex model. This phenomenon is known as **underfitting**.\n",
    "\n",
    "On the other hand, as we discussed above, we want to watch out for the cases when our training error is significantly lower than our validation error, indicating severe **overfitting**. Note that overfitting is not always a bad thing. With deep learning especially, it is well known that the best predictive models often perform far better on training data than on holdout data. Ultimately, we usually care more about the validation error than about the gap between the training and validation errors.\n",
    "    \n",
    " <img src='images\\dive.png'>\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "     Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. pp 146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.477622Z",
     "start_time": "2020-11-11T00:35:59.475501Z"
    }
   },
   "outputs": [],
   "source": [
    "#history_model.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'blue'>\n",
    "    \n",
    "Evaluation of the learning. \n",
    "    \n",
    "This is done using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.493623Z",
     "start_time": "2020-11-11T00:35:59.479124Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = linear_model.evaluate(x=test_x, y=test_y)\n",
    "\n",
    "print (\"Loss = \" + str(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "# <center> ---The following is a new model--- </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:46:25.160502Z",
     "start_time": "2020-10-19T18:46:25.155595Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Second model: The output $y$ does not depend linearly with the features. \n",
    "This fact is modeled with a sigmoid type function; for example, a hyperbolic tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "\n",
    "The features that determine the phenomenon are described by the vector  $X = (x_1, x_2, x_3, ...x_k,...x_K)$\n",
    "    \n",
    "The our model assumes that the output y varies linearly with each feature\n",
    "    $$ z = \\sum_{k=1}^K w_k*x_k + b$$\n",
    "    $$ F(X) = tanh(z)= \\frac{{e}^{2z} - 1}{{e}^{2z} + 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.655334Z",
     "start_time": "2020-11-11T00:35:59.494732Z"
    }
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(2*z)- 1)/(np.exp(2*z)+1)\n",
    "\n",
    "# The following array is generated for plotting the hyperbolic tangent function\n",
    "x1 = np.arange(-2, 2.0, 0.1)\n",
    "y1 = 1.759*tanh((2/3*x1))\n",
    "\n",
    "y2 = x1\n",
    "#Samples and function F are plotted\n",
    "plt.figure(figsize=(13,8))\n",
    "\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('legend', fontsize=16)\n",
    "plt.ylabel('Y', fontsize=16)\n",
    "plt.xlabel('Z', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('Sigmoid-type = 1.7159*tanh((2/3*x)', size=20)\n",
    "\n",
    "#Plotting function\n",
    "plt.plot(x1, y1, color='green', lw=4)\n",
    "plt.plot(x1, y2, color='red', lw=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.718829Z",
     "start_time": "2020-11-11T00:35:59.656877Z"
    }
   },
   "outputs": [],
   "source": [
    "n_x = train_x.shape[1] \n",
    "n_y = train_y.shape[1]\n",
    "    \n",
    "layers = [n_x, n_y]\n",
    "net = Network(layers)\n",
    "net.graph(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model architecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.741499Z",
     "start_time": "2020-11-11T00:35:59.721154Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "input_nodes = n_x     # The input layer has n_x nodes\n",
    "output_nodes = n_y    # The output layer has n_y nodes\n",
    "\n",
    "sigmoid_model = Sequential()\n",
    "\n",
    "sigmoid_model.add(Dense(output_nodes,  kernel_initializer='uniform', bias_initializer='zeros', \\\n",
    "                input_dim=input_nodes, activation='tanh'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:55:20.867257Z",
     "start_time": "2020-10-19T18:55:20.859036Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "Architecture Summary and Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.842995Z",
     "start_time": "2020-11-11T00:35:59.742835Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(sigmoid_model, to_file='sigmoid_model.png', show_shapes=True, rankdir='TB', \n",
    "      expand_nested=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.848492Z",
     "start_time": "2020-11-11T00:35:59.844463Z"
    }
   },
   "outputs": [],
   "source": [
    "sigmoid_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5  color='blue'>\n",
    "    \n",
    "Compiling the model. Includes the optimizer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:35:59.887698Z",
     "start_time": "2020-11-11T00:35:59.849776Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01)\n",
    "\n",
    "sigmoid_model.compile(loss='mean_squared_error', optimizer=sgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Training the learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:10.047487Z",
     "start_time": "2020-11-11T00:35:59.888965Z"
    }
   },
   "outputs": [],
   "source": [
    "# 10 % of the training data will be used to validate the training\n",
    "validation_portion = 0.1\n",
    "epochs = 600\n",
    "\n",
    "history = sigmoid_model.fit(train_x, train_y, epochs=epochs, validation_split = validation_portion, verbose=2)\n",
    "\n",
    "# the \"history\" object contains information generated during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Plots of cost function versus epoch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:10.193820Z",
     "start_time": "2020-11-11T00:36:10.049096Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history.history['loss'], color='red')\n",
    "plt.plot(history.history['val_loss'], color='green')\n",
    "plt.title('Cost function')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cost_train', 'cost_validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "# <center> Now we will construct new non linear models: Artificial Neural Networks </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T13:48:16.441234Z",
     "start_time": "2020-11-10T13:48:16.437127Z"
    }
   },
   "source": [
    "<font size=5 color='black'>\n",
    "<center> SEM images of a neuron and a network of neurons. Neuron model and mathematical model of a neuron </center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Neuron</td>\n",
    "     <td>Network of neurons</td>\n",
    "      <td>Neuron model</td>\n",
    "      <td>Mathematical model of a neuron</td>\n",
    "         \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images\\neuron_SEM.jpg\" width=290 height=480></td>\n",
    "    <td><img src=\"images\\human-neuron.png\" width=270 height=480></td>\n",
    "    <td><img src=\"images\\Neuron_labelled.png\" width=200 height=380></td>\n",
    "    <td><img src=\"images\\neuron-mat-model.png\" width=370 height=380></td>\n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T13:48:16.441234Z",
     "start_time": "2020-11-10T13:48:16.437127Z"
    }
   },
   "source": [
    "<font size=5 color='black'>\n",
    "<center> Approximation by Superpositions of a Sigmoidal Function </center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "$\\bf Abstract$-In this paper we demonstrate that finite linear combinations of com-\n",
    "positions of a fixed, univariate function and a set ofaffine functionals can uniformly\n",
    "approximate any continuous function of n real variables with support in the unit\n",
    "hypercube; only mild conditions are imposed on the univariate function. Our\n",
    "results settle an open question about representability in the class of single bidden\n",
    "layer neural networks. In particular, we show that arbitrary decision regions can\n",
    "be arbitrarily well approximated by continuous feedforward neural networks with\n",
    "only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The\n",
    "paper discusses approximation properties of other possible types of nonlinearities\n",
    "that might be implemented by artificial neural networks.\n",
    "    \n",
    "[Reference](./Literatura/Approx-superpositions-sigmoids_1989.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T13:48:16.441234Z",
     "start_time": "2020-11-10T13:48:16.437127Z"
    }
   },
   "source": [
    "<font size=5 color='black'>\n",
    "<center> Approximation Capabilities of Multilayer Feedforward Networks </center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='black'>\n",
    "$\\bf Abstract$--We show that standard multilayer feedfbrward networks with as few as a single hidden layer and\n",
    "arbitrary bounded and nonconstant activation function are universal approximators with respect to LP(lt) per-\n",
    "formance criteria, for arbitrary finite input environment measures p, provided only that sufficiently many hidden\n",
    "units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings\n",
    "can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks\n",
    "with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a_Function and\n",
    "its derivatives.\n",
    "    \n",
    "[Reference](./Literatura/FF-NN-universal-Approximator_1991.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model using three neurons. Full-Connected Feedforward Network (FF). The activation function of the last neuron is linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:10.261734Z",
     "start_time": "2020-11-11T00:36:10.195493Z"
    }
   },
   "outputs": [],
   "source": [
    "n_x = train_x.shape[1] \n",
    "n_h = 2\n",
    "n_y = train_y.shape[1]\n",
    "    \n",
    "layers = [n_x, n_h, n_y]\n",
    "net = Network(layers)\n",
    "net.graph(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model architecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:10.296004Z",
     "start_time": "2020-11-11T00:36:10.263079Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "input_nodes = n_x     # The input layer has n_x nodes\n",
    "hlayer1_nodes = n_h   # The first hidden layer has n_h nodes\n",
    "output_nodes = n_y    # The output layer has n_y nodes\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Dense(hlayer1_nodes,  kernel_initializer='uniform', bias_initializer='zeros', \\\n",
    "                input_dim=input_nodes, activation='tanh'))\n",
    "\n",
    "model1.add(Dense(output_nodes, kernel_initializer='uniform', bias_initializer='zeros', activation='linear'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:55:20.867257Z",
     "start_time": "2020-10-19T18:55:20.859036Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "Architecture Summary and Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:10.374912Z",
     "start_time": "2020-11-11T00:36:10.297442Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model1, to_file='model1.png', show_shapes=True, rankdir='TB', \n",
    "      expand_nested=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:10.382039Z",
     "start_time": "2020-11-11T00:36:10.377259Z"
    }
   },
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5  color='blue'>\n",
    "    \n",
    "Compiling the model. Includes the optimizer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:10.418114Z",
     "start_time": "2020-11-11T00:36:10.383801Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01)\n",
    "\n",
    "model1.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Training the learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:19.468081Z",
     "start_time": "2020-11-11T00:36:10.419800Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_portion = 0.1\n",
    "epochs = 600\n",
    "\n",
    "history = model1.fit(train_x, train_y, epochs=epochs, validation_split = validation_portion, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Plots of cost function versus epoch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:19.608496Z",
     "start_time": "2020-11-11T00:36:19.469237Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history.history['loss'], color='red')\n",
    "plt.plot(history.history['val_loss'], color='green')\n",
    "plt.title('Cost function')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cost_train', 'cost_validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "A good model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "# <center> ---The following is a new model--- </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model using three neurons. The activation function of the last neuron is sigmoid type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:19.684816Z",
     "start_time": "2020-11-11T00:36:19.610657Z"
    }
   },
   "outputs": [],
   "source": [
    "n_x = train_x.shape[1] \n",
    "n_h = 2\n",
    "n_y = train_y.shape[1]\n",
    "    \n",
    "layers = [n_x, n_h, n_y]\n",
    "net = Network(layers)\n",
    "net.graph(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model architecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:19.719144Z",
     "start_time": "2020-11-11T00:36:19.686189Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "input_nodes = n_x     # The input layer has n_x nodes\n",
    "hlayer1_nodes = n_h   # The first hidden layer has n_h nodes\n",
    "output_nodes = n_y    # The output layer has n_y nodes\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(hlayer1_nodes,  kernel_initializer='uniform', bias_initializer='zeros', \\\n",
    "                input_dim=input_nodes, activation='tanh'))\n",
    "\n",
    "model2.add(Dense(output_nodes, kernel_initializer='uniform', bias_initializer='zeros', activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:55:20.867257Z",
     "start_time": "2020-10-19T18:55:20.859036Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "Architecture Summary and Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:19.798324Z",
     "start_time": "2020-11-11T00:36:19.720543Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model2, to_file='model2.png', show_shapes=True, rankdir='TB', \n",
    "      expand_nested=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:19.803927Z",
     "start_time": "2020-11-11T00:36:19.800138Z"
    }
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5  color='blue'>\n",
    "    \n",
    "Compiling the model. Includes the optimizer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:19.839393Z",
     "start_time": "2020-11-11T00:36:19.805536Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01)\n",
    "\n",
    "model2.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Training the learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:28.836790Z",
     "start_time": "2020-11-11T00:36:19.841097Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_portion = 0.1\n",
    "epochs = 600\n",
    "\n",
    "history = model2.fit(train_x, train_y, epochs=epochs, validation_split = validation_portion, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Plots of cost function versus epoch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:28.975756Z",
     "start_time": "2020-11-11T00:36:28.838007Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history.history['loss'], color='red')\n",
    "plt.plot(history.history['val_loss'], color='green')\n",
    "plt.title('Cost function')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cost_train', 'cost_validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "# <center> ---The following is a new model--- </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model using four neurons. The activation function of the last neuron is linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:29.044923Z",
     "start_time": "2020-11-11T00:36:28.977297Z"
    }
   },
   "outputs": [],
   "source": [
    "n_x = train_x.shape[1] \n",
    "n_h = 3\n",
    "n_y = train_y.shape[1]\n",
    "    \n",
    "layers = [n_x, n_h, n_y]\n",
    "net = Network(layers)\n",
    "net.graph(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model architecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:29.080209Z",
     "start_time": "2020-11-11T00:36:29.046514Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "input_nodes = n_x     #input layer has n_x nodes\n",
    "hlayer1_nodes = n_h   #first hidden layer has n_h nodes\n",
    "output_nodes = n_y    #output layer has n_y nodes\n",
    "\n",
    "model3.add(Dense(hlayer1_nodes,  kernel_initializer='uniform', bias_initializer='zeros', \\\n",
    "                input_dim=input_nodes, activation='tanh'))\n",
    "\n",
    "model3.add(Dense(output_nodes, kernel_initializer='uniform', bias_initializer='zeros', activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:55:20.867257Z",
     "start_time": "2020-10-19T18:55:20.859036Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "Architecture Summary and Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:29.159360Z",
     "start_time": "2020-11-11T00:36:29.081488Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model3, to_file='model3.png', show_shapes=True, rankdir='TB', \n",
    "      expand_nested=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:29.165429Z",
     "start_time": "2020-11-11T00:36:29.161053Z"
    }
   },
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5  color='blue'>\n",
    "    \n",
    "Compiling the model. Includes the optimizer definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:29.201395Z",
     "start_time": "2020-11-11T00:36:29.166888Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01)\n",
    "\n",
    "model3.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Training the learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:38.110153Z",
     "start_time": "2020-11-11T00:36:29.202761Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_portion = 0.1\n",
    "epochs = 600\n",
    "\n",
    "history = model3.fit(train_x, train_y, epochs=epochs, validation_split = validation_portion, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Plots of cost function versus epoch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:38.256723Z",
     "start_time": "2020-11-11T00:36:38.111481Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history.history['loss'], color='red')\n",
    "plt.plot(history.history['val_loss'], color='green')\n",
    "plt.title('Cost function')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cost_train', 'cost_validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "# <center> ---The following is a new model--- </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model using five neurons. The activation function of the last neuron is linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:38.330139Z",
     "start_time": "2020-11-11T00:36:38.258301Z"
    }
   },
   "outputs": [],
   "source": [
    "n_x = train_x.shape[1] \n",
    "n_h = 4\n",
    "n_y = train_y.shape[1]\n",
    "    \n",
    "layers = [n_x, n_h, n_y]\n",
    "net = Network(layers)\n",
    "net.graph(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model architecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:38.367409Z",
     "start_time": "2020-11-11T00:36:38.331857Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "input_nodes = n_x     # The input layer has n_x nodes\n",
    "hlayer1_nodes = n_h   # The first hidden layer has n_h nodes\n",
    "output_nodes = n_y    # The output layer has n_y nodes\n",
    "\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Dense(hlayer1_nodes,  kernel_initializer='uniform', bias_initializer='zeros', \\\n",
    "                input_dim=input_nodes, activation='tanh'))\n",
    "\n",
    "model4.add(Dense(output_nodes, kernel_initializer='uniform', bias_initializer='zeros', activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:55:20.867257Z",
     "start_time": "2020-10-19T18:55:20.859036Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "Architecture Summary and Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:38.450037Z",
     "start_time": "2020-11-11T00:36:38.368787Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model4, to_file='model4.png', show_shapes=True, rankdir='TB', \n",
    "      expand_nested=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:38.455051Z",
     "start_time": "2020-11-11T00:36:38.451443Z"
    }
   },
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5  color='blue'>\n",
    "    \n",
    "Compiling the model. Includes the optimizer definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:38.491568Z",
     "start_time": "2020-11-11T00:36:38.456334Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01,)\n",
    "\n",
    "model4.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Training the learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:47.626197Z",
     "start_time": "2020-11-11T00:36:38.492776Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_portion = 0.1\n",
    "epochs = 600\n",
    "\n",
    "history = model4.fit(train_x, train_y, epochs=epochs, validation_split = validation_portion,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Plots of cost function versus epoch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:47.767678Z",
     "start_time": "2020-11-11T00:36:47.627416Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history.history['loss'], color='red')\n",
    "plt.plot(history.history['val_loss'], color='green')\n",
    "plt.title('Cost function')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cost_train', 'cost_validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Overfitting   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "# <center> ---The following is a new model--- </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model using ten neurons. The activation function of the last neuron is linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:47.843320Z",
     "start_time": "2020-11-11T00:36:47.769210Z"
    }
   },
   "outputs": [],
   "source": [
    "n_x = train_x.shape[1] \n",
    "n_h1 = 5\n",
    "n_h2  = 4\n",
    "n_y = train_y.shape[1]\n",
    "    \n",
    "layers = [n_x, n_h1, n_h2, n_y]\n",
    "net = Network(layers)\n",
    "net.graph(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Model architecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:47.891769Z",
     "start_time": "2020-11-11T00:36:47.844983Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "input_nodes = n_x     # The input layer has n_x nodes\n",
    "hlayer1_nodes = n_h1  # The first hidden layer has n_h1 nodes\n",
    "hlayer2_nodes = n_h2  # The second hidden layes has n_h2 nodes\n",
    "output_nodes = n_y    # The output layer has n_y nodes\n",
    "\n",
    "\n",
    "model5 = Sequential()\n",
    "\n",
    "model5.add(Dense(hlayer1_nodes,  kernel_initializer='uniform', bias_initializer='zeros', \\\n",
    "                input_dim=input_nodes, activation='tanh'))\n",
    "\n",
    "model5.add(Dense(hlayer2_nodes,  kernel_initializer='uniform', bias_initializer='zeros', \\\n",
    "                input_dim=input_nodes, activation='tanh'))\n",
    "\n",
    "\n",
    "model5.add(Dense(output_nodes, kernel_initializer='uniform', bias_initializer='zeros', activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:55:20.867257Z",
     "start_time": "2020-10-19T18:55:20.859036Z"
    }
   },
   "source": [
    "<font size=5 color='blue'>\n",
    "Architecture Summary and Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:47.978233Z",
     "start_time": "2020-11-11T00:36:47.896723Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model5, to_file='model5.png', show_shapes=True, rankdir='TB', \n",
    "      expand_nested=True, show_layer_names=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:47.985276Z",
     "start_time": "2020-11-11T00:36:47.980549Z"
    }
   },
   "outputs": [],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5  color='blue'>\n",
    "    \n",
    "Compiling the model. Includes the optimizer definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:48.022958Z",
     "start_time": "2020-11-11T00:36:47.986531Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01,)\n",
    "\n",
    "model5.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "    \n",
    "Training the learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:58.257554Z",
     "start_time": "2020-11-11T00:36:48.024246Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_portion = 0.1\n",
    "epochs = 600\n",
    "\n",
    "history = model5.fit(train_x, train_y, epochs=epochs, validation_split = validation_portion,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Plots of cost function versus epoch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T00:36:58.407673Z",
     "start_time": "2020-11-11T00:36:58.258785Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(history.history['loss'], color='red')\n",
    "plt.plot(history.history['val_loss'], color='green')\n",
    "plt.title('Cost function')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['cost_train', 'cost_validation'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color='blue'>\n",
    "\n",
    "Overfitting    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
